{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import os\n",
    "import concurrent.futures\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Maximum number of retries\n",
    "MAX_RETRIES = 10\n",
    "\n",
    "# Delay between retries in seconds\n",
    "RETRY_DELAY = 1\n",
    "\n",
    "# Define the maximum number of concurrent threads\n",
    "MAX_WORKERS = 3\n",
    "\n",
    "FUTURES = []\n",
    "\n",
    "TYPE_ELECTIONS = [\n",
    "    [\"pilpres\", \"PILPRES\"],\n",
    "    [\"pilegdpr\", \"PILEG DPR\"],\n",
    "    [\"pilegdprd_prov\", \"PILEG DPRD PROVINSI\"],\n",
    "    [\"pilegdprd_kab\", \"PILEG DPRD KAB KOTA\"],\n",
    "    [\"pemilu_dpd\", \"PEMILU DPD\"],\n",
    "]\n",
    "\n",
    "url_base = \"https://sirekap-obj-data.kpu.go.id/wilayah/pemilu/ppwp\"\n",
    "url_chart = \"https://sirekap-obj-data.kpu.go.id/pemilu/hhcw/pdpr\"\n",
    "today_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "def generate_url_form(type: str):\n",
    "    return f\"https://pemilu2024.kpu.go.id/{type}/hitung-suara/wilayah\"\n",
    "\n",
    "\n",
    "def ensure_directory_exists(directory_path):\n",
    "    \"\"\"\n",
    "    Ensure that a directory exists. If it does not exist, create it.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): The path of the directory to ensure existence.\n",
    "\n",
    "    Returns:\n",
    "        str: The path of the directory.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "    return directory_path\n",
    "\n",
    "\n",
    "def custom_logger(today_date: str):\n",
    "    # Configure the logging settings\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    # Create a FileHandler to write log messages to a file\n",
    "    ensure_directory_exists(f\"./logs\")\n",
    "    log_file = f\"./logs/{today_date}_app.log\"\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    # Set the log level for the file handler\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "\n",
    "    # Create a Formatter to specify the log message format\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    # Create a logger object and add the FileHandler to it\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    # Return the logger object\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = custom_logger(today_date)\n",
    "\n",
    "\n",
    "def getJSON(url: str):\n",
    "    headers = {\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-GB,en;q=0.6',\n",
    "        'Cache-Control': 'no-cache',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Origin': 'https://pemilu2024.kpu.go.id',\n",
    "        'Pragma': 'no-cache',\n",
    "        'Referer': 'https://pemilu2024.kpu.go.id/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-site',\n",
    "        'Sec-GPC': '1',\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',\n",
    "        'sec-ch-ua': '\"Not A(Brand\";v=\"99\", \"Brave\";v=\"121\", \"Chromium\";v=\"121\"',\n",
    "        'sec-ch-ua-mobile': '?0',\n",
    "        'sec-ch-ua-platform': '\"Windows\"',\n",
    "    }\n",
    "\n",
    "    response = requests.get(url=url, headers=headers)\n",
    "    response = response.json()\n",
    "    return response\n",
    "\n",
    "\n",
    "def crawl_website(url: str):\n",
    "    # Set up Selenium WebDriver\n",
    "    # Path to chromedriver executable\n",
    "    service = Service(\"./chrome-driver/chromedriver.exe\")\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "\n",
    "    # Load the webpage\n",
    "    driver.get(url)\n",
    "\n",
    "    # Execute a user script to click the button\n",
    "    user_script = \"\"\"\n",
    "        const btn = document.querySelector(\"button.btn.btn-dark.float-end\");\n",
    "        btn.click();\n",
    "    \"\"\"\n",
    "    driver.execute_script(user_script)\n",
    "\n",
    "    # Wait for the page to fully render\n",
    "    driver.implicitly_wait(10)  # Adjust the wait time as needed\n",
    "\n",
    "    # Extract image links\n",
    "    def extract_image_links(driver):\n",
    "        # Find all links on the page\n",
    "        links = driver.find_elements(By.TAG_NAME, 'a')\n",
    "\n",
    "        # Extract image links only\n",
    "        image_links = []\n",
    "\n",
    "        for link in links:\n",
    "            href = link.get_attribute('href')\n",
    "            if href and any(ext in href.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif']):\n",
    "                image_links.append(href)\n",
    "\n",
    "        if not image_links:\n",
    "            # Capture the result of the recursive call\n",
    "            image_links += extract_image_links(driver)\n",
    "\n",
    "        return image_links\n",
    "\n",
    "    images = extract_image_links(driver)\n",
    "    # Quit the WebDriver\n",
    "    driver.quit()\n",
    "    return images\n",
    "\n",
    "\n",
    "def find_in_array_of_json(array, key, value):\n",
    "    \"\"\"\n",
    "    Find the first element in the array of JSON objects where the specified key has the given value.\n",
    "\n",
    "    Args:\n",
    "        array (list): The array of JSON objects to search through.\n",
    "        key (str): The key to search for.\n",
    "        value (any): The value to search for in the specified key.\n",
    "\n",
    "    Returns:\n",
    "        dict or None: The first JSON object in the array where the specified key has the given value,\n",
    "                      or None if no such element is found.\n",
    "    \"\"\"\n",
    "    for item in array:\n",
    "        if item.get(key) == value:\n",
    "            return item\n",
    "    return None\n",
    "\n",
    "\n",
    "def generate_url(url_base: str, components_uri: list, is_json: bool = True):\n",
    "    \"\"\"\n",
    "    Generate a URL by concatenating components with the base URL.\n",
    "\n",
    "    Args:\n",
    "        url_base (str): The base URL.\n",
    "        components_uri (list): List of components to append to the base URL.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated URL.\n",
    "    \"\"\"\n",
    "    if is_json:\n",
    "        return url_base + \"/\" + \"/\".join(components_uri) + \".json\"\n",
    "    else:\n",
    "        return url_base + \"/\" + \"/\".join(components_uri)\n",
    "\n",
    "\n",
    "def filter_array_of_dicts(array_of_dicts, key, value):\n",
    "    \"\"\"\n",
    "    Filter an array of dictionaries based on a given key-value pair.\n",
    "\n",
    "    Args:\n",
    "        array_of_dicts (list): The array of dictionaries to filter.\n",
    "        key (str): The key to filter on.\n",
    "        value (any): The value to filter for.\n",
    "\n",
    "    Returns:\n",
    "        list: The filtered list of dictionaries.\n",
    "    \"\"\"\n",
    "    return [d for d in array_of_dicts if d.get(key) == value]\n",
    "\n",
    "\n",
    "def download_images(image_links, output_path, overwrite=False):\n",
    "    \"\"\"\n",
    "    Download images from the provided links.\n",
    "\n",
    "    Args:\n",
    "        image_links (list): List of image URLs to download.\n",
    "        output_path (str): Path to the directory where the downloaded images will be saved.\n",
    "        overwrite (bool, optional): Whether to overwrite images if they already exist in the output directory. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        list: List of filenames of successfully downloaded images.\n",
    "    \"\"\"\n",
    "    # Create the output directory if it does not exist\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    downloaded_images = []\n",
    "\n",
    "    # Download images and save them to the output directory\n",
    "    for i, image_link in enumerate(image_links):\n",
    "        # Assuming images are JPEGs\n",
    "        image_filename = os.path.join(output_path, f\"image_{i+1}.jpg\")\n",
    "        if not overwrite and os.path.exists(image_filename):\n",
    "            logger.info(\n",
    "                f\"Image already exists : '{image_filename}'. Skipping...\")\n",
    "            downloaded_images.append(image_filename)\n",
    "            continue\n",
    "\n",
    "        attempt = 0\n",
    "        while attempt < MAX_RETRIES:\n",
    "            try:\n",
    "                response = requests.get(image_link)\n",
    "                with open(image_filename, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                downloaded_images.append(image_filename)\n",
    "                break  # Break out of the retry loop if download is successful\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to download image {image_link}: {e}\")\n",
    "                attempt += 1\n",
    "                logger.error(f\"Retrying ({attempt}/{MAX_RETRIES})...\")\n",
    "                time.sleep(RETRY_DELAY)\n",
    "        else:\n",
    "            logger.error(\n",
    "                f\"Failed to download image {image_link} after {MAX_RETRIES} attempts.\")\n",
    "\n",
    "    return downloaded_images\n",
    "\n",
    "\n",
    "def process_tps(url_website, tps_dir):\n",
    "    # Crawl website to get image links\n",
    "    image_links = crawl_website(url=url_website)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    # Check if images are already downloaded\n",
    "    if os.path.isdir(tps_dir) and len(os.listdir(tps_dir)) == len(image_links):\n",
    "        logger.info(f\"Images already downloaded at {tps_dir}. Skipping...\")\n",
    "    else:\n",
    "        # Download images\n",
    "        downloaded_images = download_images(image_links, tps_dir)\n",
    "        \n",
    "        # Check if all images are downloaded successfully\n",
    "        if len(downloaded_images) == len(image_links):\n",
    "            logger.info(f'Images downloaded to \"{tps_dir}\"')\n",
    "        else:\n",
    "            logger.error(f\"Couldn't download all images to '{tps_dir}'\")\n",
    "\n",
    "\n",
    "def process_village(output_path:str, provincy:dict, regency:dict, district:dict, village:dict, election:str):\n",
    "    village_dir = os.path.join(\n",
    "                    output_path, provincy['nama'], regency['nama'], district['nama'], village['nama'])\n",
    "    tps = getJSON(generate_url(url_base=url_base, components_uri=[\n",
    "        provincy['kode'], regency['kode'], district['kode'], village['kode']]))\n",
    "    chart = getJSON(generate_url(url_base=url_chart, components_uri=[\n",
    "                    provincy['kode'], regency['kode'], district['kode'], village['kode']]))\n",
    "    filtered_tps = [current_tps for current_tps in tps if current_tps['kode'] in chart['table'] and chart['table'][current_tps['kode']].get(\"1\") is not None and chart['table'][current_tps['kode']]['status_progress']]\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        for current_tps in filtered_tps:\n",
    "            tps_dir = os.path.join(village_dir, current_tps[\"nama\"])\n",
    "            url_website = generate_url(url_base=generate_url_form(election), components_uri=[\n",
    "                provincy['kode'], regency['kode'], district['kode'], village['kode'], current_tps['kode']], is_json=False)\n",
    "            executor.submit(process_tps, url_website, tps_dir)\n",
    "        executor.shutdown(wait=True)\n",
    "      \n",
    "\n",
    "def main(provincy_id: str, regency_id: str, index_type_election: int):\n",
    "    provinces = getJSON(generate_url(url_base=url_base, components_uri=[\"0\"]))\n",
    "    provincy = find_in_array_of_json(provinces, \"kode\", provincy_id)\n",
    "    regencies = getJSON(generate_url(url_base=url_base,\n",
    "                        components_uri=[provincy['kode']]))\n",
    "\n",
    "    output_path = os.path.join(\n",
    "        os.getcwd(), \"output\", TYPE_ELECTIONS[index_type_election][1])\n",
    "    ensure_directory_exists(output_path)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        regency = find_in_array_of_json(regencies, \"kode\", regency_id)\n",
    "        districts = getJSON(generate_url(url_base=url_base, components_uri=[\n",
    "                            provincy['kode'], regency['kode']]))\n",
    "        for district in districts:\n",
    "            villages = getJSON(generate_url(url_base=url_base, components_uri=[\n",
    "                provincy_id, regency_id, district['kode']]))\n",
    "            for village in villages:\n",
    "                executor.submit(process_village, output_path, provincy, regency, district, village, TYPE_ELECTIONS[index_type_election][0])\n",
    "        executor.shutdown(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PILPRES (0)\n",
      "PILEG DPR (1)\n",
      "PILEG DPRD PROVINSI (2)\n",
      "PILEG DPRD KAB KOTA (3)\n",
      "PEMILU DPD (4)\n",
      "ACEH (11)\n",
      "BALI (51)\n",
      "BANTEN (36)\n",
      "BENGKULU (17)\n",
      "DAERAH ISTIMEWA YOGYAKARTA (34)\n",
      "DKI JAKARTA (31)\n",
      "GORONTALO (75)\n",
      "JAMBI (15)\n",
      "JAWA BARAT (32)\n",
      "JAWA TENGAH (33)\n",
      "JAWA TIMUR (35)\n",
      "KALIMANTAN BARAT (61)\n",
      "KALIMANTAN SELATAN (63)\n",
      "KALIMANTAN TENGAH (62)\n",
      "KALIMANTAN TIMUR (64)\n",
      "KALIMANTAN UTARA (65)\n",
      "KEPULAUAN BANGKA BELITUNG (19)\n",
      "KEPULAUAN RIAU (21)\n",
      "LAMPUNG (18)\n",
      "Luar Negeri (99)\n",
      "MALUKU (81)\n",
      "MALUKU UTARA (82)\n",
      "NUSA TENGGARA BARAT (52)\n",
      "NUSA TENGGARA TIMUR (53)\n",
      "P A P U A (91)\n",
      "PAPUA BARAT (92)\n",
      "PAPUA BARAT DAYA (96)\n",
      "PAPUA PEGUNUNGAN (95)\n",
      "PAPUA SELATAN (93)\n",
      "PAPUA TENGAH (94)\n",
      "RIAU (14)\n",
      "SULAWESI BARAT (76)\n",
      "SULAWESI SELATAN (73)\n",
      "SULAWESI TENGAH (72)\n",
      "SULAWESI TENGGARA (74)\n",
      "SULAWESI UTARA (71)\n",
      "SUMATERA BARAT (13)\n",
      "SUMATERA SELATAN (16)\n",
      "SUMATERA UTARA (12)\n",
      "BANDUNG (3204)\n",
      "BANDUNG BARAT (3217)\n",
      "BEKASI (3216)\n",
      "BOGOR (3201)\n",
      "CIAMIS (3207)\n",
      "CIANJUR (3203)\n",
      "CIREBON (3209)\n",
      "GARUT (3205)\n",
      "INDRAMAYU (3212)\n",
      "KARAWANG (3215)\n",
      "KOTA BANDUNG (3273)\n",
      "KOTA BANJAR (3279)\n",
      "KOTA BEKASI (3275)\n",
      "KOTA BOGOR (3271)\n",
      "KOTA CIMAHI (3277)\n",
      "KOTA CIREBON (3274)\n",
      "KOTA DEPOK (3276)\n",
      "KOTA SUKABUMI (3272)\n",
      "KOTA TASIKMALAYA (3278)\n",
      "KUNINGAN (3208)\n",
      "MAJALENGKA (3210)\n",
      "PANGANDARAN (3218)\n",
      "PURWAKARTA (3214)\n",
      "SUBANG (3213)\n",
      "SUKABUMI (3202)\n",
      "SUMEDANG (3211)\n",
      "TASIKMALAYA (3206)\n"
     ]
    }
   ],
   "source": [
    "def multiline_input(lines: list = [], prompt=\"\"):\n",
    "    for line in lines:\n",
    "        print(f\"{line['nama']} ({line['kode']})\")\n",
    "    selected_item = input(prompt)\n",
    "    return selected_item\n",
    "\n",
    "selected_type_election_index = multiline_input(lines=[{\"nama\": x[1], \"kode\": i} for i, x in enumerate(\n",
    "    TYPE_ELECTIONS)], prompt=\"Pilih Tipe Pemilu : \")\n",
    "\n",
    "if selected_type_election_index:\n",
    "    selected_type_election = TYPE_ELECTIONS[int(selected_type_election_index)]\n",
    "    provincies = getJSON(generate_url(url_base, [\"0\"]))\n",
    "    selected_provincy = multiline_input(lines=provincies, prompt=\"Pilih Kode Provinsi : \")\n",
    "    \n",
    "    if selected_provincy:\n",
    "        regencies = getJSON(generate_url(url_base, [selected_provincy]))\n",
    "        selected_regency = multiline_input(lines=regencies, prompt=\"Pilih Kode Kecamatan : \")\n",
    "        \n",
    "        if selected_regency:\n",
    "            main(selected_provincy, selected_regency, int(selected_type_election_index))\n",
    "        else:\n",
    "            logger.error(\"Pilih Kode Kecamatan..\")\n",
    "    else:\n",
    "        logger.error(\"Pilih Kode Provinsi..\")\n",
    "else:\n",
    "    logger.error(\"Pilih Tipe Pemilu..\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
