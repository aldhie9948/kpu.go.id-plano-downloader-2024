{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By \n",
    "import os\n",
    "import concurrent.futures\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Maximum number of retries\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "# Delay between retries in seconds\n",
    "RETRY_DELAY = 3\n",
    "\n",
    "# Define the maximum number of concurrent threads\n",
    "MAX_WORKERS = 5\n",
    "\n",
    "\n",
    "TYPE_ELECTIONS = [\n",
    "    [\"pilpres\", \"PILPRES\"],\n",
    "    [\"pilegdpr\", \"PILEG DPR\"],\n",
    "    [\"pilegdprd_prov\", \"PILEG DPRD PROVINSI\"],\n",
    "    [\"pilegdprd_kab\", \"PILEG DPRD KAB KOTA\"],\n",
    "    [\"pemilu_dpd\", \"PEMILU DPD\"],\n",
    "]\n",
    "\n",
    "\n",
    "url_base = \"https://sirekap-obj-data.kpu.go.id/wilayah/pemilu/ppwp\"\n",
    "url_chart = \"https://sirekap-obj-data.kpu.go.id/pemilu/hhcw/pdpr\"\n",
    "today_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "def generate_url_form(type: str):\n",
    "    return f\"https://pemilu2024.kpu.go.id/{type}/hitung-suara/wilayah\"\n",
    "\n",
    "\n",
    "def ensure_directory_exists(directory_path):\n",
    "    \"\"\"\n",
    "    Ensure that a directory exists. If it does not exist, create it.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): The path of the directory to ensure existence.\n",
    "\n",
    "    Returns:\n",
    "        str: The path of the directory.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "    return directory_path\n",
    "\n",
    "\n",
    "def custom_logger(today_date: str):\n",
    "    # Configure the logging settings\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    # Create a FileHandler to write log messages to a file\n",
    "    ensure_directory_exists(f\"./logs\")\n",
    "    log_file = f\"./logs/{today_date}_app.log\"\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    # Set the log level for the file handler\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "\n",
    "    # Create a Formatter to specify the log message format\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    # Create a logger object and add the FileHandler to it\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    # Return the logger object\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = custom_logger(today_date)\n",
    "\n",
    "\n",
    "def getJSON(url: str):\n",
    "    headers = {\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'en-GB,en;q=0.6',\n",
    "        'Cache-Control': 'no-cache',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Origin': 'https://pemilu2024.kpu.go.id',\n",
    "        'Pragma': 'no-cache',\n",
    "        'Referer': 'https://pemilu2024.kpu.go.id/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-site',\n",
    "        'Sec-GPC': '1',\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',\n",
    "        'sec-ch-ua': '\"Not A(Brand\";v=\"99\", \"Brave\";v=\"121\", \"Chromium\";v=\"121\"',\n",
    "        'sec-ch-ua-mobile': '?0',\n",
    "        'sec-ch-ua-platform': '\"Windows\"',\n",
    "    }\n",
    "\n",
    "    response = requests.get(url=url, headers=headers)\n",
    "    response = response.json()\n",
    "    return response\n",
    "\n",
    "def crawl_website(url: str):\n",
    "    # Set up Selenium WebDriver\n",
    "    # Path to chromedriver executable\n",
    "    service = Service(\"./chrome-driver/chromedriver.exe\")\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "\n",
    "    # Load the webpage\n",
    "    driver.get(url)\n",
    "\n",
    "    # Execute a user script to click the button\n",
    "    user_script = \"\"\"\n",
    "        const btn = document.querySelector(\"button.btn.btn-dark.float-end\");\n",
    "        btn.click();\n",
    "    \"\"\"\n",
    "    driver.execute_script(user_script)\n",
    "\n",
    "    # Wait for the page to fully render\n",
    "    driver.implicitly_wait(10)  # Adjust the wait time as needed\n",
    "\n",
    "    # Extract image links\n",
    "    def extract_image_links(driver):\n",
    "        # Find all links on the page\n",
    "        links = driver.find_elements(By.TAG_NAME, 'a')\n",
    "\n",
    "        # Extract image links only\n",
    "        image_links = []\n",
    "\n",
    "        for link in links:\n",
    "            href = link.get_attribute('href')\n",
    "            if href and any(ext in href.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif']):\n",
    "                image_links.append(href)\n",
    "\n",
    "        if not image_links:\n",
    "            # Capture the result of the recursive call\n",
    "            image_links += extract_image_links(driver)\n",
    "\n",
    "        return image_links\n",
    "\n",
    "    images = extract_image_links(driver)\n",
    "    # Quit the WebDriver\n",
    "    driver.quit()\n",
    "    return images\n",
    "\n",
    "def find_in_array_of_json(array, key, value):\n",
    "    \"\"\"\n",
    "    Find the first element in the array of JSON objects where the specified key has the given value.\n",
    "\n",
    "    Args:\n",
    "        array (list): The array of JSON objects to search through.\n",
    "        key (str): The key to search for.\n",
    "        value (any): The value to search for in the specified key.\n",
    "\n",
    "    Returns:\n",
    "        dict or None: The first JSON object in the array where the specified key has the given value,\n",
    "                      or None if no such element is found.\n",
    "    \"\"\"\n",
    "    for item in array:\n",
    "        if item.get(key) == value:\n",
    "            return item\n",
    "    return None\n",
    "\n",
    "def generate_url(url_base: str, components_uri: list, is_json: bool = True):\n",
    "    \"\"\"\n",
    "    Generate a URL by concatenating components with the base URL.\n",
    "\n",
    "    Args:\n",
    "        url_base (str): The base URL.\n",
    "        components_uri (list): List of components to append to the base URL.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated URL.\n",
    "    \"\"\"\n",
    "    if is_json:\n",
    "        return url_base + \"/\" + \"/\".join(components_uri) + \".json\"\n",
    "    else:\n",
    "        return url_base + \"/\" + \"/\".join(components_uri)\n",
    "\n",
    "def filter_array_of_dicts(array_of_dicts, key, value):\n",
    "    \"\"\"\n",
    "    Filter an array of dictionaries based on a given key-value pair.\n",
    "\n",
    "    Args:\n",
    "        array_of_dicts (list): The array of dictionaries to filter.\n",
    "        key (str): The key to filter on.\n",
    "        value (any): The value to filter for.\n",
    "\n",
    "    Returns:\n",
    "        list: The filtered list of dictionaries.\n",
    "    \"\"\"\n",
    "    return [d for d in array_of_dicts if d.get(key) == value]\n",
    "\n",
    "def download_images(image_links, output_path, overwrite=False):\n",
    "    \"\"\"\n",
    "    Download images from the provided links.\n",
    "\n",
    "    Args:\n",
    "        image_links (list): List of image URLs to download.\n",
    "        output_path (str): Path to the directory where the downloaded images will be saved.\n",
    "        overwrite (bool, optional): Whether to overwrite images if they already exist in the output directory. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        list: List of filenames of successfully downloaded images.\n",
    "    \"\"\"\n",
    "    # Create the output directory if it does not exist\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    downloaded_images = []\n",
    "\n",
    "    # Download images and save them to the output directory\n",
    "    for i, image_link in enumerate(image_links):\n",
    "        # Assuming images are JPEGs\n",
    "        image_filename = os.path.join(output_path, f\"image_{i+1}.jpg\")\n",
    "        if not overwrite and os.path.exists(image_filename):\n",
    "            logger.info(\n",
    "                f\"Image '{image_filename}' already exists. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        attempt = 0\n",
    "        while attempt < MAX_RETRIES:\n",
    "            try:\n",
    "                response = requests.get(image_link)\n",
    "                with open(image_filename, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                downloaded_images.append(image_filename)\n",
    "                break  # Break out of the retry loop if download is successful\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to download image {image_link}: {e}\")\n",
    "                attempt += 1\n",
    "                logger.error(f\"Retrying ({attempt}/{MAX_RETRIES})...\")\n",
    "                time.sleep(RETRY_DELAY)\n",
    "        else:\n",
    "            logger.error(\n",
    "                f\"Failed to download image {image_link} after {MAX_RETRIES} attempts.\")\n",
    "\n",
    "    return downloaded_images\n",
    "\n",
    "def process_tps(url_website, output_path, provincy, regency, district, village, tps):\n",
    "    image_links = crawl_website(url=url_website)\n",
    "    time.sleep(0.5)\n",
    "    output = os.path.join(\n",
    "        output_path, provincy['nama'], regency['nama'], district['nama'], village['nama'], tps['nama'])\n",
    "    downloaded_images = download_images(\n",
    "        image_links=image_links, output_path=output)\n",
    "    if len(downloaded_images) == len(image_links):\n",
    "        logger.info(f'Downloading images to \"{output}\"')\n",
    "    else:\n",
    "        process_tps(url_website, output_path, provincy,\n",
    "                    regency, district, village, tps)\n",
    "\n",
    "def main(provincy_id: str, regency_id: str, index_type_election: int):\n",
    "    provinces = getJSON(generate_url(url_base=url_base, components_uri=[\"0\"]))\n",
    "    provincy = find_in_array_of_json(provinces, \"kode\", provincy_id)\n",
    "    regencies = getJSON(generate_url(url_base=url_base,\n",
    "                        components_uri=[provincy['kode']]))\n",
    "\n",
    "    output_path = os.path.join(\n",
    "        os.getcwd(), \"output\", TYPE_ELECTIONS[index_type_election][1])\n",
    "    ensure_directory_exists(output_path)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = []\n",
    "        regency = find_in_array_of_json(regencies, \"kode\", regency_id)\n",
    "        districts = getJSON(generate_url(url_base=url_base, components_uri=[\n",
    "                            provincy['kode'], regency['kode']]))\n",
    "        for district in districts:\n",
    "            villages = getJSON(generate_url(url_base=url_base, components_uri=[\n",
    "                                provincy_id, regency_id, district['kode']]))\n",
    "            for village in villages:\n",
    "                tps = getJSON(generate_url(url_base=url_base, components_uri=[\n",
    "                                provincy['kode'], regency['kode'], district['kode'], village['kode']]))\n",
    "                chart = getJSON(generate_url(url_base=url_chart, components_uri=[\n",
    "                                provincy['kode'], regency['kode'], district['kode'], village['kode']]))\n",
    "                for current_tps in tps:\n",
    "                    current_chart = chart['table'][current_tps['kode']]\n",
    "                    if (current_chart['status_progress'] == False or current_chart.get(\"1\") == None):\n",
    "                        continue\n",
    "                    url_website = generate_url(url_base=generate_url_form(TYPE_ELECTIONS[index_type_election][0]), components_uri=[\n",
    "                                                provincy['kode'], regency['kode'], district['kode'], village['kode'], current_tps['kode']], is_json=False)\n",
    "                    \n",
    "                    futures.append(\n",
    "                        executor.submit(\n",
    "                            process_tps,\n",
    "                            url_website, output_path, provincy, regency, district, village, current_tps\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "    # Wait for all futures to complete\n",
    "    concurrent.futures.wait(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def multiline_input(lines: list = [], prompt=\"\",):\n",
    "    print(\"\\n\")\n",
    "    for x in lines:\n",
    "        print(f\"{x['nama']} ({x['kode']})\")\n",
    "    return input(prompt)\n",
    "\n",
    "\n",
    "selected_type_election = multiline_input(lines=[{\"nama\": x[1], \"kode\": i} for i, x in enumerate(\n",
    "    TYPE_ELECTIONS)], prompt=\"Pilih Tipe Pemilu : \")\n",
    "if (selected_type_election):\n",
    "    provincy_id = multiline_input(lines=getJSON(generate_url(\n",
    "        url_base, [\"0\"])), prompt=\"Pilih Kode Provinsi : \")\n",
    "    if not provincy_id:\n",
    "        logger.error(\"Pilih Kode Provinsi..\")\n",
    "    else:\n",
    "        regency_id = multiline_input(lines=getJSON(generate_url(\n",
    "            url_base, [provincy_id])), prompt=\"Pilih Kode Kecamatan : \")\n",
    "        if not regency_id:\n",
    "            logger.error(\"Pilih Kode Kecamatan..\")\n",
    "        else:\n",
    "            main(provincy_id, regency_id, int(selected_type_election))\n",
    "else:\n",
    "    logger.error(\"Pilih Tipe Pemilu..\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
